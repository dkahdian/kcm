\documentclass[10pt]{article}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xcolor}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{mathtools}
\usepackage{cleveref}
\usepackage{natbib} 
\usepackage{url}
\usepackage{fourier}
\usepackage{algorithmic}
\usepackage{algorithm}
\definecolor{lightskyblue}{rgb}{0.53, 0.81, 0.98}\newcommand{\oliver}[1]{{\small{\color{lightskyblue}[Oliver: #1]}}}
\usepackage[margin=1in,bottom=1in,top=1in]{geometry}
\usepackage{palatino}
\usetikzlibrary{calc}


% -------- Theorem styles --------
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}{Claim}[section]

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\newcommand{\var}{\text{var}}

\title{Executive Summary for Updated Knowledge Compilation Map}
\author{David Kahdian}
\begin{document}
\maketitle

\begin{abstract}
We describe the algorithmic machinery behind the Updated Knowledge Compilation Map, an interactive tool for exploring succinctness relations and query tractability across KC languages. Starting from a hand-curated knowledge base of known results, we validate consistency via DFS-based reachability, then propagate derived facts through fixed-point upgrade and contradiction-driven downgrade algorithms. Query and transformation support is propagated analogously using succinctness relations and operation implication lemmas. We then encode the knowledge base as a propositional CNF formula, compile it to d-DNNF using c2d, and verify that the entailed literals match the graph-based propagation exactly---confirming soundness and completeness of our algorithms. The compiled representation further enables model counting ($\approx 2^{30.45}$ consistent worlds) and surprise analysis, ranking open problems by their information content. The project is publicly visible at \url{https://kcm.kahdian.com} and on Github at \url{https://github.com/dkahdian/kcm}.
\end{abstract}

\section{Setup}

\subsection{Data}
Data was primarily sourced from \citet{Darwiche_2002}, \citet{Amarilli_2018}, \citet{Bova_2016}, and related works. The knowledge base stores: (1)~a set of KC languages~$V$, (2)~a directed adjacency matrix of succinctness relations on $V \times V$, and (3)~per-language query/transformation support tables.

Each directed edge $L_i \to L_j$ carries one of six statuses capturing the existence or non-existence of polynomial (P) and quasi-polynomial (Q) compilations:
\begin{center}
\small
\begin{tabular}{lcc}
\textbf{Status} & \textbf{P exists?} & \textbf{Q exists?} \\ \hline
\texttt{poly} & \checkmark & \checkmark \\
\texttt{unknown-poly-quasi} & ? & \checkmark \\
\texttt{no-poly-quasi} & $\times$ & \checkmark \\
\texttt{unknown-both} & ? & ? \\
\texttt{no-poly-unknown-quasi} & $\times$ & ? \\
\texttt{no-quasi} & $\times$ & $\times$ \\
\end{tabular}
\end{center}

Query/transformation entries carry the same status vocabulary: each operation on each language records whether it can be performed in polynomial time, quasi-polynomial time, or not at all.

\subsection{Transitivity Lemmas}

The propagation logic relies on two families of lemmas.

\paragraph{Succinctness transitivity.}
If $L_1 \leq_p L_2$ and $L_2 \leq_p L_3$, then $L_1 \leq_p L_3$ (similarly for $\leq_q$). By contrapositive: if $L_1 \not\leq_p L_3$, then $L_1 \not\leq_p L_2$ or $L_2 \not\leq_p L_3$.
 
\paragraph{Query-by-compiling.}
If $L_1 \leq_p L_2$ and $L_2$ supports query~$q$ in polytime, then $L_1$ supports $q$ in polytime (compile first, then query). Contrapositive: if $L_1$ cannot perform~$q$ in polytime and $L_1 \leq_p L_2$, then $L_2$ cannot perform~$q$ in polytime either.

\paragraph{Operation implication lemmas.}
A fixed set of ``hardcoded'' lemmas of the form $O_1 \wedge \cdots \wedge O_n \Rightarrow O_{n+1}$ hold within any single language. For example, $\text{CO} \wedge \lnot\text{C} \Rightarrow \text{VA}$. Their contrapositives are also exploited.

\section{Validation}

Represent the knowledge base as a directed graph $G = (V, E)$ with $|V| = v \approx 20$ languages and $|E| = e \approx 400$ edges. Each edge carries a positive label (compilation exists) or a negative label (compilation does not exist).

\textbf{Succinctness validation.} A contradiction occurs when a positive path (chain of ``exists'' edges) connects $L_i$ to $L_j$ but the direct edge $L_i \to L_j$ is negative. We run a DFS from every vertex on the subgraph of positive edges, yielding $O(v(v + e))$ time. Witness paths for contradictions are extracted from the DFS parent pointers.

\textbf{Query/transformation validation.} Two checks: (1)~for each language and each operation lemma $O_1 \wedge \cdots \wedge O_n \Rightarrow O_{n+1}$, verify that it is not the case that all antecedents are supported while the consequent is marked unsupported ($O(vL)$ for $L \approx 10$ lemmas); (2)~for each pair $(L_1, L_2)$ with $L_1 \leq_p L_2$, verify that no query $q$ is supported on $L_2$ but marked impossible on $L_1$ ($O(v^2 Q)$ for $Q \approx 10$ queries).

\section{Succinctness Propagation}

The propagation algorithm fills in derived relations that follow logically from the hand-entered data. It operates in two phases, both run to a fixed point.

\subsection{Phase 1: Upgrades (Fixed-Point)}

An \emph{upgrade} strengthens an edge status---for example, marking an unknown edge as \texttt{poly}. The core idea is transitive closure: we repeatedly compute the reachability matrix on the subgraph of polytime edges (via DFS from every vertex), then for each pair $(i,j)$ that is reachable but whose direct edge does not yet reflect this, we upgrade $i \to j$ to \texttt{poly} and record the witness path from the DFS parent pointers. If an upgrade would contradict a known negative edge, a contradiction is raised.

This repeats until no further upgrades are possible (fixed point). Each iteration costs $O(v(v+e))$ for the DFS. Since each edge can only be upgraded a bounded number of times and there are $O(v^2)$ edges, the loop runs at most $O(v^2)$ iterations for a total of $O(v^3(v+e))$, though in practice it converges in 2--3 passes. The quasi-polynomial case is handled identically with a broader set of qualifying edge statuses.

\subsection{Phase 2: Downgrades (Fixed-Point)}

A \emph{downgrade} narrows an edge status based on contradiction. The algorithm tentatively assumes a stronger status for an edge and checks (using the validator) whether that would create a path contradicting a known negative edge. If so, the stronger status is ruled out.

Concretely, for each edge $i \to j$ with status \texttt{null} or \texttt{unknown-both}:
\begin{enumerate}
\item Tentatively set $i \to j$ to \texttt{poly} and run the consistency checker. If a contradiction is found, downgrade to \texttt{no-poly-unknown-quasi} (``poly is impossible'').
\item If poly was not eliminated, tentatively set $i \to j$ to \texttt{unknown-poly-quasi} and run consistency. If contradiction, downgrade to \texttt{no-quasi}.
\end{enumerate}

Similarly, \texttt{unknown-poly-quasi} edges are tested for a poly upgrade (yielding \texttt{no-poly-quasi} on failure), and \texttt{no-poly-unknown-quasi} edges are tested for a quasi upgrade (yielding \texttt{no-quasi} on failure).

This phase also runs to a fixed point because a downgrade on one edge may enable further downgrades elsewhere. Each trial consistency check costs $O(v(v+e))$, and there are $O(v^2)$ edges to test per iteration, giving $O(v^2 \cdot v(v+e))$ per round.

Witness paths from the DFS are stored as human-readable proof descriptions on each derived edge, enabling the UI to display \emph{why} a relation was inferred.

\section{Query/Transformation Propagation}

After succinctness propagation, we propagate query and transformation support across languages in three sub-phases.

\subsection{Phase 3: Upgrades via Succinctness}

Using the query-by-compiling principle: if $L_1 \leq_p L_2$ (from the now-complete reachability matrix) and $L_2$ supports query $q$ in polytime, then $L_1$ also supports $q$ in polytime. The quasi analogue holds as well. Process poly upgrades first (to a fixed point), then quasi upgrades. Cost: $O(v^2 Q)$ per iteration.

\subsection{Phase 4: Upgrades via Lemmas}

For each language $L$ and each operation lemma $O_1 \wedge \cdots \wedge O_n \Rightarrow O_{n+1}$: if $L$ supports all antecedents in polytime, mark the consequent as polytime-supported (similarly for quasi, with complexity bounded by the worst antecedent). This is interleaved with Phase~3 in the same fixed-point loop. Cost: $O(vL)$ per iteration.

\subsection{Phase 5: Downgrades}

\paragraph{Via succinctness (contrapositive of query-by-compiling).}
If $L_1$ cannot perform $q$ in polytime and $L_1 \leq_p L_2$, then $L_2$ cannot perform $q$ in polytime. (If it could, $L_1$ would compile to $L_2$ and query there---contradiction.) The quasi analogue applies with $\leq_q$. Cost: $O(v^2 Q)$.

\paragraph{Via lemma contrapositives.}
Given $O_1 \wedge \cdots \wedge O_n \Rightarrow O_{n+1}$, if the consequent $O_{n+1}$ is impossible (say, no-poly) and all antecedents except $O_j$ are supported, then $O_j$ must be impossible too. Each lemma is checked against every language. Cost: $O(vLn)$.

Phases 3--5 are each repeated to a fixed point, since one inference may enable another. All derived entries are tagged \texttt{derived = true} with human-readable proof descriptions and references.

\section{Meta-Knowledge Compilation}

The knowledge base itself can be cast as a propositional satisfiability problem. We have implemented this encoding and compiled it to d-DNNF, enabling model counting and surprise analysis over the space of consistent worlds.

\subsection{Encoding}

We restrict attention to the $v = 21$ active languages (excluding 7~in-progress languages that lack any known edges). For each ordered pair $(i,j)$ with $i \neq j$, we introduce two Boolean variables: $P_{ij}$ (a polytime compilation $L_i \to L_j$ exists) and $Q_{ij}$ (a quasi-polytime compilation exists), for a total of $2 \cdot v(v-1) = 840$ variables. The invariant $P_{ij} \Rightarrow Q_{ij}$ is encoded as $(\lnot P_{ij} \lor Q_{ij})$.

Known facts become unit clauses: for instance, a \texttt{poly} edge $i \to j$ asserts $P_{ij} = \top$; a \texttt{no-quasi} edge asserts $Q_{ij} = \bot$. Edge statuses that leave one component unknown (e.g.\ \texttt{no-poly-unknown-quasi} constrains $P_{ij} = \bot$ but leaves $Q_{ij}$ free) contribute a single unit clause.

The transitivity lemmas yield 3-literal clauses over all triples, for both the polynomial and quasi-polynomial layers:
\[
  \forall\, i,j,k:\quad (\lnot P_{ij} \lor \lnot P_{jk} \lor P_{ik}) \qquad\text{and}\qquad (\lnot Q_{ij} \lor \lnot Q_{jk} \lor Q_{ik})
\]

The resulting formula~$\Sigma$ has 840~variables and 17{,}121~clauses: 420 implication clauses ($P \Rightarrow Q$), 741 unit clauses from known facts, and 15{,}960 transitivity clauses.

\subsection{Compilation and Results}

The CNF was compiled to a smooth d-DNNF using c2d~v2.20 in approximately 14~seconds, producing a 243{,}170-node circuit (4.5\,MB). From this compiled representation we extract:

\begin{itemize}
  \item \textbf{Model count.} $\text{MC}(\Sigma) = 1{,}466{,}882{,}601 \approx 2^{30.45}$ consistent worlds.
  \item \textbf{Clausal entailment.} Of the 840~variables, 241~are entailed true, 500~are entailed false, and 99~are free (unknown).
  \item \textbf{Verification.} The set of entailed literals from the d-DNNF matches exactly the set of known and derived facts from the graph-based propagation algorithms of Sections~3--4. There are zero discrepancies in either direction, confirming empirically that the algorithmic propagation is both \emph{sound} (it derives nothing beyond what the axioms entail) and \emph{complete} (it derives everything the axioms entail).
\end{itemize}

\subsection{Surprise Analysis}

For each free variable~$v$ and truth value $\textit{val} \in \{\top, \bot\}$, the \emph{surprise} of the hypothetical result $v = \textit{val}$ is:
\[
  \text{surprise}(v = \textit{val}) \;=\; \log_2 \frac{\text{MC}(\Sigma)}{\text{MC}(\Sigma \mid v = \textit{val})}
\]
Higher surprise means the result eliminates a larger fraction of consistent worlds. Table~\ref{tab:surprise} lists the 20~most informative open problems. The cSDD language dominates the ranking because it is a recently introduced language with few known succinctness relationships, making each potential result highly constraining.

\begin{table}[ht]
\centering
\small
\caption{Top 20 most informative open problems by maximum surprise (bits). $\text{s}(\top)$ and $\text{s}(\bot)$ denote the surprise of the variable being true or false, respectively. By nature, it is highly unlikely that any of these results holds; however, if we could resolve even one, it would yield a large amount of information about the structure of the KC landscape.}
\label{tab:surprise}
\begin{tabular}{clrr}
\textbf{Rank} & \textbf{Variable} & $\mathbf{s(\top)}$ & $\mathbf{s(\bot)}$ \\ \hline
1 & $P(\text{dec-SDNNF} \to \text{cSDD})$ & 11.84 & 0.00 \\
2 & $P(\text{SDD} \to \text{cSDD})$ & 11.81 & 0.00 \\
3 & $P(\text{IP} \to \text{cSDD})$ & 11.55 & 0.00 \\
4 & $P(\text{uOBDD} \to \text{cSDD})$ & 11.35 & 0.00 \\
5 & $Q(\text{DNF} \to \text{cSDD})$ & 10.48 & 0.00 \\
6 & $P(\text{cSDD} \to \text{MODS})$ & 9.49 & 0.00 \\
7 & $Q(\text{d-SDNNF} \to \text{cSDD})$ & 8.00 & 0.01 \\
8 & $Q(\text{uOBDD} \to \text{cSDD})$ & 8.00 & 0.01 \\
9 & $Q(\text{IP} \to \text{cSDD})$ & 7.99 & 0.01 \\
10 & $Q(\text{SDD} \to \text{cSDD})$ & 7.15 & 0.01 \\
11 & $P(\text{nOBDD} \to \text{FBDD})$ & 6.89 & 0.01 \\
12 & $P(\text{SDNNF} \to \text{dec-DNNF})$ & 6.88 & 0.01 \\
13 & $P(\text{OBDD} \to \text{cSDD})$ & 6.87 & 0.01 \\
14 & $P(\text{DNF} \to \text{nOBDD})$ & 5.77 & 0.03 \\
15 & $P(\text{IP} \to \text{uOBDD})$ & 5.35 & 0.04 \\
16 & $Q(\text{cSDD} \to \text{FBDD})$ & 0.04 & 5.35 \\
17 & $Q(\text{cSDD} \to \text{dec-DNNF})$ & 0.04 & 5.35 \\
18 & $P(\text{OBDD}_< \to \text{cSDD})$ & 5.24 & 0.04 \\
19 & $Q(\text{OBDD} \to \text{cSDD})$ & 4.98 & 0.05 \\
20 & $Q(\text{dec-SDNNF} \to \text{cSDD})$ & 4.98 & 0.05 \\
\end{tabular}
\end{table}

\subsection{Research Value}

The surprise metric from Table~\ref{tab:surprise} ranks variables by how informative a single direction is, but many open problems are highly asymmetric: one outcome is very surprising while the other is nearly trivial.  To capture the value of investigating an open problem \emph{regardless of outcome}, we define the \textbf{research value}:
\[
  \text{RV}(v) \;=\; \text{surprise}(v = \top) \;\cdot\; \text{surprise}(v = \bot)
\]
A high research value means that both possible outcomes are roughly equally likely (in the sense that the model count of each outcome is similar), so resolving the problem is highly informative no matter the answer and worth investigating.

Unlike the max-surprise ranking, which is dominated by cSDD-related variables with one-sided surprise, the research value ranking surfaces \emph{balanced} open problems---those where the outcome is genuinely uncertain from the perspective of our current knowledge. All top entries have research values near~1.0, corresponding to roughly equal surprise in both directions.

\begin{table}[ht]
\centering
\small
\caption{Top 20 open problems by research value $\text{RV} = \text{s}(\top) \cdot \text{s}(\bot)$.}
\label{tab:rv}
\begin{tabular}{clrrr}
\textbf{Rank} & \textbf{Variable} & $\mathbf{s(\top)}$ & $\mathbf{s(\bot)}$ & \textbf{RV} \\ \hline
1 & $Q(\text{cSDD} \to \text{IP})$ & 0.98 & 1.02 & 1.00 \\
2 & $P(\text{cSDD} \to \text{DNF})$ & 1.09 & 0.91 & 1.00 \\
3 & $Q(\text{d-SDNNF} \to \text{SDD})$ & 0.90 & 1.11 & 1.00 \\
4 & $Q(\text{uOBDD} \to \text{SDD})$ & 0.90 & 1.11 & 1.00 \\
5 & $P(\text{IP} \to \text{SDNNF})$ & 0.88 & 1.13 & 1.00 \\
6 & $P(\text{uOBDD} \to \text{dec-DNNF})$ & 1.14 & 0.87 & 0.99 \\
7 & $Q(\text{cSDD} \to \text{OBDD}_<)$ & 1.15 & 0.86 & 0.99 \\
8 & $P(\text{cSDD} \to \text{FBDD})$ & 1.19 & 0.83 & 0.99 \\
9 & $P(\text{nOBDD} \to \text{d-DNNF})$ & 0.83 & 1.20 & 0.99 \\
10 & $P(\text{cSDD} \to \text{CNF})$ & 1.20 & 0.82 & 0.99 \\
11 & $Q(\text{MODS} \to \text{PI})$ & 0.81 & 1.22 & 0.99 \\
12 & $Q(\text{DNF} \to \text{nOBDD})$ & 1.25 & 0.79 & 0.98 \\
13 & $Q(\text{DNF} \to \text{SDNNF})$ & 1.25 & 0.79 & 0.98 \\
14 & $Q(\text{d-SDNNF} \to \text{FBDD})$ & 0.77 & 1.27 & 0.98 \\
15 & $Q(\text{d-SDNNF} \to \text{dec-DNNF})$ & 0.77 & 1.27 & 0.98 \\
16 & $Q(\text{uOBDD} \to \text{FBDD})$ & 0.77 & 1.27 & 0.98 \\
17 & $Q(\text{uOBDD} \to \text{dec-DNNF})$ & 0.77 & 1.27 & 0.98 \\
18 & $P(\text{SDD} \to \text{dec-DNNF})$ & 1.29 & 0.76 & 0.98 \\
19 & $P(\text{DNF} \to \text{nFBDD})$ & 1.29 & 0.76 & 0.98 \\
20 & $P(\text{SDD} \to \text{nFBDD})$ & 0.75 & 1.30 & 0.98 \\
\end{tabular}
\end{table}

\subsection{Comparison to Algorithmic Propagation}

The dedicated graph algorithms of Sections~3--4 remain preferable for the web application for two reasons: (1)~they produce \emph{informative witness paths}---human-readable chains of citations and intermediate edges explaining \emph{why} each derived fact holds---whereas a compiled representation only answers ``entailed or not''; and (2)~they are simpler and faster. The meta-compilation approach complements these algorithms by enabling queries they do not support (model counting, surprise quantification) and by providing an independent verification that the propagation is sound and complete.

\clearpage
% =============================
% Bibliography
% =============================
\bibliographystyle{plainnat}
\bibliography{refs}

\end{document}