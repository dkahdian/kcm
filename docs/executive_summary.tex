\documentclass[10pt]{article}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xcolor}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{mathtools}
\usepackage{cleveref}
\usepackage{natbib} 
\usepackage{url}
\usepackage{fourier}
\usepackage{algorithmic}
\usepackage{algorithm}
\definecolor{lightskyblue}{rgb}{0.53, 0.81, 0.98}\newcommand{\oliver}[1]{{\small{\color{lightskyblue}[Oliver: #1]}}}
\usepackage[margin=1in,bottom=1in,top=1in]{geometry}
\usepackage{palatino}
\usetikzlibrary{calc}


% -------- Theorem styles --------
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}{Claim}[section]

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\newcommand{\var}{\text{var}}

\title{Executive Summary for Updated Knowledge Compilation Map}
\author{David Kahdian}
\begin{document}
\maketitle

\begin{abstract}
We describe the Updated Knowledge Compilation Map. This project is meant as a modernization of \citet{Darwiche_2002}. At the lowest level, the project is a survey of the Knowledge Compilation landscape. We collected only "direct" results, that is, results that are not derived via transitive relations from other results. Internally, we represent these results as a graph, and use a DFS reachability algorithm to validate consistency. We use this algorithm to build a propagator, which uses transitive and other simple lemmas to derive related results. This gives us expanded versions of each of the tables in \citet{Darwiche_2002}.

To further validate our results, we encode the knowledge base as a propositional CNF formula, compile it to d-DNNF using $\text{c2d}$, and verify that the entailed literals match the graph-based propagation exactly. The compiled representation further enables model counting and surprise analysis, ranking open problems by their information content. The project is publicly visible at \url{https://kcm.kahdian.com} and on Github at \url{https://github.com/dkahdian/kcm}.
\end{abstract}

\section{Setup}

\subsection{Data representation}
Data was primarily sourced from \citet{Darwiche_2002}, \citet{Amarilli_2018}, \citet{Bova_2016}, and related works.

The knowledge base stores a set of KC languages~$\textbf{L}$ and some relations on $\textbf{L}$ as defined below.

Each succinctness relationship carries one of six statuses capturing the existence or non-existence of polynomial (P) and quasi-polynomial (Q) compilations. For instance, $\texttt{no-poly-unknown-quasi}$ implies that no polynomial compilation is possible, but a quasipolynomial compilation has not been shown or ruled out. Let $S$ be the set of statuses listed below:
\begin{center}
\small
\begin{tabular}{lcc}
\textbf{Status} & \textbf{P exists?} & \textbf{Q exists?} \\ \hline
\texttt{poly} & \checkmark & \checkmark \\
\texttt{unknown-poly-quasi} & ? & \checkmark \\
\texttt{no-poly-quasi} & $\times$ & \checkmark \\
\texttt{unknown-both} & ? & ? \\
\texttt{no-poly-unknown-quasi} & $\times$ & ? \\
\texttt{no-quasi} & $\times$ & $\times$ \\
\end{tabular}
\end{center}

Thus, the matrix of succinctness relations $M_s$ may be represented as a mapping $M_s:\textbf{L} \times \textbf{L} \to \textbf{S}$, where $M_s(L_1,L_2)$ represents the complexity of compiling from $L_1$ to $L_2$  We use succinctness relations $\leq_p, \leq_q$ in the expected way; for instance $L_2 \leq_q L_1 \iff M_s(L_1, L_2) \in \{\texttt{poly},\texttt{no-poly-quasi},\texttt{unknown-poly-quasi}\}$. For the rest of this paper, we will focus only on $\tilde S = \{ \texttt{poly}, \texttt{no-poly}, \texttt{unknown}\}$. All algorithms and logic for $\tilde S$ is easily extended to $S$.


Query/transformation entries carry the same status vocabulary $\tilde S$, but instead represent the \emph{tractability} of an operation on a language. Thus, if $\textbf{O}=\textbf{Q} \cup \textbf{T}$ is the set of all operations we store, the operations matrix may be represented as a mapping $M_o:\textbf{L} \times \textbf{O} \to \textbf{S}$. 

For simplicity we use the notation $L.o = M_o(L,o)$

We say a language $L \in \textbf{L}$ \emph{supports} an operation $o \in \textbf{O}$ in status $s \in \tilde S$ if $L.o=\texttt{poly}$, and \emph{is unsupported} $o$ if $M_o(L,o)=\texttt{no-poly}$.

Note that "not supported" (which could be \texttt{no-poly} or \texttt{unknown}) is distinct from "unsupported" (which means strictly \texttt{no-poly}).

\paragraph{Caveats.}
Many \texttt{no-poly} results are conditional on complexity-theoretic assumptions such as $\text{P} \neq \text{NP}$ or the polynomial hierarchy not collapsing. Each entry in $M_s$ or $M_o$ may carry an optional \emph{caveat} string $c$ (e.g.\ ``$\text{P} = \text{NP}$''), meaning the result holds \emph{unless} $c$ is true. When a derived result has multiple unique caveats, these are merged by union. As such, the handling of the caveats in the algorithms below is as expected. 

However, the current implementation does have a small flaw: the propagator does not "prioritize" unconditional results over conditional results, so we may end up with conditional results that could be upgraded to unconditional. Implementing a system that prioritizes unconditional results is simple: we would only need to double the status space $S$ and thus also double the runtime.



\subsection{Transitivity Lemmas}

The propagation logic relies on the following simple lemmas. We always assume $L_1,L_2 \in \textbf{L}, q \in Q, t \in T$.

\paragraph{Succinctness transitivity.}
\begin{itemize}
  \item{If $L_1 \leq_p L_2$ and $L_2 \leq_p L_3$, then $L_1 \leq_p L_3$.}
  \item{Corollary: if $L_1 \not\leq_p L_3$, then either $L_1 \not\leq_p L_2$ or $L_2 \not\leq_p L_3$.}
\end{itemize}
 
\paragraph{Query-by-compiling}
\begin{itemize}
\item{If $L_1 \leq_p L_2$ and $L_2$ supports $q$, then $L_1$ supports $q$ (compile, then query).}
\item{Corollary: if $L_1$ does not support $q$, but $L_1 \leq_p L_2$, then $L_2$ does not support $q$ either.}
\item{Corollary: if $L_1$ supports $q$, but $L_2$ does not support $q$, then $L_2$ cannot be compiled to $L_1$.}
\end{itemize}

Warning: a similar result does not apply for transformations.

\paragraph{Operation implication lemmas.}
We additionally have a \emph{family} of lemmas $F_l \subset \mathcal{P}(O) \times O$ of the form $o_1 \wedge \cdots \wedge o_n \Rightarrow o_{n+1}$, for $o_i\in \textbf{O}$, in the sense that if a language $L$ supports all the operations $o_1, \cdots, o_n$, it also supports $o_{n+1}$. For example, $\text{CO} \wedge \lnot\text{C} \Rightarrow \text{VA}$. Their contrapositives are also exploited, i.e. $\neg(\text{VA}) \wedge (\neg \text{C}) \Rightarrow \neg(\text{CO})$. These lemmas are obvious from definitions and are mostly sourced from \citet{Darwiche_2002}. 

\section{Validation}

\textbf{Motivation.} The "validator" checks whether the data represented by the knowledge base is consistent with the above lemmas. We wish to have this algorithm to check contributions and as a subroutine for the propagator, described later.

\textbf{Description.} The validator is formally a mapping $(M_s, M_o) \mapsto \{\top, \bot\}$. The propagator benefits from explanations of \emph{why} a knowledge base is invalid, so our algorithm will return a plain-text description of the contradiction as well.


\textbf{Succinctness validation.} Represent the succinctness relationships as a graph $G = (\textbf{L}, E)$, where $(L_1, L_2, s) \in E \iff M_s(L_1, L_2) = s$. There is a contradiction if and only if, for languages $L_i, L_j$, there exists a path of $\texttt{poly}$ edges while $M_s(L_i, L_j) = \texttt{no-poly}$. A DFS finds any such contradictions, with the corresponding witness path, in $O((\#\textbf{L})^2)$ time.

\textbf{Query-by-compile validation.} We consider each pair of languages $L_i, L_j \in \textbf{L}$ so that a path of $\texttt{poly}$ edges exists from $L_i$ to $L_j$. For such pairs, we ensure that every query $q$ supported by $L_2$ is not unsupported by $L_1$. This requires a similar DFS algorithm taking $O((\#\textbf{L})^2(\#Q)$ time.

\textbf{Implication lemma validation.} For each language $L \in \textbf{L}$ and for each lemma $l=(\{o_1, ..., o_n\}, o_{n+1}) \in F_l$, we check that $L.o_{n+1})=\texttt{no-poly}) \wedge (\bigwedge_{i=1}^n  L.o_i=\texttt{no-poly})$ does not hold. This takes $O((\#\textbf{L})(\#F_l)(\#\textbf{O}))$ time.

\section{Propagation}

\paragraph{Motivation.} The propagation algorithm fills in derived relations that follow logically from the hand-entered data. This is useful for the following purposes:
\begin{itemize}
    \item{Reducing manual labor, both in terms of data entry and thinking about transitivity}
    \item{Provides intuitive explanations for how claims hold.}
    \item{Allows researchers to easily check the implications of any assumed relationship}
\end{itemize}

\paragraph{Description.} The propagator can be viewed as a mapping $(M_s, M_o) \mapsto (\widetilde M_s, \widetilde M_o)$ which replaces $\texttt{unknown}$ edges with $\texttt{poly}$ or $\texttt{no-poly}$ edges based on the lemmas in \textbf{1.2}. The algorithms below are complete in the sense that they infer all possible information, although we have not drafted proofs of this.

\subsection{Phase 1: Succinctness Upgrades (Fixed-Point)}

An \emph{upgrade} changes an \texttt{unknown} to \texttt{poly}. We use a fixed-point DFS-based transitive closure algorithm. We find paths of \texttt{poly} edges between languages $L_i, L_j$, and map $M_s(L_i, L_j, \texttt{unknown})$ to $M_s(L_i, L_j, \texttt{poly})$. We then use that witness path to write a plaintext explanation of how the upgrade was derived.

This repeats until no further upgrades are possible (fixed point). Each iteration costs $O((\#\textbf{L})^2)$ for the DFS. Since each edge can only be upgraded a bounded number of times and there are $O((\#\textbf{L})^2))$ unknown edges, the loop runs at most $O((\#\textbf{L})^2))$ iterations for a total of $O((\#\textbf{L})^4))$, though in practice it converges in only 2--3 passes.

Note that, when handling the full set of statuses $S$, we apply the algorithm to upgrade to $\texttt{poly}$ first, then to $\texttt{quasi}$, in separate fixed-point iterations.

\subsection{Phase 2: Succinctness Downgrades (Fixed-Point)}

A \emph{downgrade} changes an \texttt{unknown} to \texttt{no-poly}. The approach is via exhaustive search: for each \texttt{unknown} edge, we temporarily set the edge to \texttt{poly} and check (using the \texttt{validator}) whether that creates a contradiction. If so, the stronger status is ruled out, and we use the contradicting path as an explanation for why.

This phase also runs to a fixed point because a downgrade on one edge may enable further downgrades elsewhere. Each trial consistency check costs $O((\#\textbf{L})^2))$, and there are $O((\#\textbf{L})^2))$ edges to test per iteration, and we must run the validator (which takes $O((\#\textbf{L})^2)))$ giving $O((\#\textbf{L})^6))$ overall runtime.

Note that, when handling $S$ instead of $\tilde S$, during each iteration of the fixed point, we first try setting the edge to $\texttt{poly}$; then, if no contradiction is created, to $\texttt{unknown-poly-quasi}$. This interleaving is critical for correctness.

\subsection{Query/Transformation Propagation}

After phase 2, $\widetilde M_s$ is finalized (ignoring Phase~6), so we move on to $\widetilde M_o$. We propagate query and transformation support across languages in three sub-phases.

\subsection{Phase 3: Upgrades via Succinctness}

This is another fixed-point algorithm using the query-by-compiling principle: if $L_1 \leq_p L_2$ (because we now have transitive closure, this is equivalent to checking all paths) and $L_2.q = \texttt{poly}$, then $L_1.q = \texttt{poly}$. Checking all triples $(L_1, L_2, q)$ takes $O((\#\textbf{L})^2(\#Q))$ time. Since there are at most $O((\#\textbf{L})^2))$ upgrades to check, this takes at most $O((\#\textbf{L})^4(\#Q))$ time, although, again, we typically converge after very few iterations.

When dealing with the full set of statuses $S$, we process \texttt{poly} upgrades first (to a fixed point), then worry about quasi-polynomial status upgrades.

\subsection{Phase 4: Upgrades via Lemmas}

For each language $L$ and each operation lemma $(\{o_1,...o_n\}, o_{n+1})$: if $L$ supports all $o_i: i \in [n]$, set $L.o_{n+1}$ to $\texttt{poly}$. Note that this is interleaved with Phase~3 in the same fixed-point loop. Cost: $O((\#F_l)(\#\textbf{L}))$ per iteration, with $O((\#F_l)(\#L))$ iterations, for a total complexity of $O((\#F_l)^2(\#\textbf{L})^2)$.

\subsection{Phase 5: Downgrades}

\paragraph{Via succinctness (contrapositive of query-by-compiling).}
If $L_1.q = \texttt{no-poly}$ and $L_1 \leq_p L_2$, then $L_2.q = \texttt{no-poly}$. (If $L_2.q = \texttt{poly}$, $L_1$ would compile to $L_2$ and query there---contradiction.) Checking this for each pair of languages and for each query takes $O((\#\textbf{L})^2)(\#Q))$ per iteration.

\paragraph{Via operation implications lemmas corollary.}
For each language $L$ and each lemma $l=(\{o_1,o_2,...,o_n\},o_{n+1})$, if we have that $L$ supports $o_i$ for all $i$ except $\tilde i$ (which forces $L.o_{\tilde i}=\texttt{unknown}$), set $L.o_{\tilde i}$ to $\texttt{no-poly}$, for otherwise the lemma would imply $L_o$ supports $o_{n+1}$. This takes $O((\#\textbf{L})(\#F_l))$ time per iteration.

In theory, this can downgrade $\texttt{unknown}$ results to $\texttt{no-poly}$ for \emph{transformations} as well as queries. Unfortunately, in practice no such downgrades are applicable.

Phases 3--5 are each repeated to a fixed point, since one inference may enable another. This takes at most $O((\#\textbf{L})^4(\#Q + \#F_l))$ time. (All derived entries are tagged \texttt{derived = true} with human-readable proof descriptions and references.)

\subsection{Phase 6: Succinctness Downgrades via Query Differences}

For each pair of languages $L_1, L_2$ and each query $q$, if $q$ is unsupported by $L_1$ but supported by $L_2$, we may set $M_s(L_1,L_2)$ to $\texttt{no-poly}$ (for else we could query $L_1$ by compiling to $L_2$). This takes at most $O((\#\textbf{L})^2(\#Q))$ time per iteration.

Since Phase~6 may alter $M_s$, which may affect earlier phases, the entire propagator (Phases~1--6) is wrapped in a global fixed-point loop, although usually this requires no more than 2 iterations to converge.

\section{Meta-Knowledge Compilation (Needs rewrite)}

As hinted above, the knowledge base itself can be cast as a propositional satisfiability problem. We have implemented this encoding and compiled it to d-DNNF, enabling model counting and surprise analysis over the space of consistent worlds.

\subsection{Encoding}

We restrict attention to the $v = 21$ active languages (excluding 7~in-progress languages that lack any known edges). For each ordered pair $(i,j)$ with $i \neq j$, we introduce two Boolean variables: $P_{ij}$ (a polytime compilation $L_i \to L_j$ exists) and $Q_{ij}$ (a quasi-polytime compilation exists), for a total of $2 \cdot v(v-1) = 840$ variables. The invariant $P_{ij} \Rightarrow Q_{ij}$ is encoded as $(\lnot P_{ij} \lor Q_{ij})$.

Known facts become unit clauses: for instance, a \texttt{poly} edge $i \to j$ asserts $P_{ij} = \top$; a \texttt{no-quasi} edge asserts $Q_{ij} = \bot$. Edge statuses that leave one component unknown (e.g.\ \texttt{no-poly-unknown-quasi} constrains $P_{ij} = \bot$ but leaves $Q_{ij}$ free) contribute a single unit clause.

The transitivity lemmas yield 3-literal clauses over all triples, for both the polynomial and quasi-polynomial layers:
\[
  \forall\, i,j,k:\quad (\lnot P_{ij} \lor \lnot P_{jk} \lor P_{ik}) \qquad\text{and}\qquad (\lnot Q_{ij} \lor \lnot Q_{jk} \lor Q_{ik})
\]

The resulting formula~$\Sigma$ has 840~variables and 17{,}121~clauses: 420 implication clauses ($P \Rightarrow Q$), 741 unit clauses from known facts, and 15{,}960 transitivity clauses.

\subsection{Compilation and Results}

The CNF was compiled to a smooth d-DNNF using c2d~v2.20 in approximately 14~seconds, producing a 243{,}170-node circuit (4.5\,MB). From this compiled representation we extract:

\begin{itemize}
  \item \textbf{Model count.} $\text{MC}(\Sigma) = 1{,}466{,}882{,}601 \approx 2^{30.45}$ consistent worlds.
  \item \textbf{Clausal entailment.} Of the 840~variables, 241~are entailed true, 500~are entailed false, and 99~are free (unknown).
  \item \textbf{Verification.} The set of entailed literals from the d-DNNF matches exactly the set of known and derived facts from the graph-based propagation algorithms of Sections~3--4. There are zero discrepancies in either direction, confirming empirically that the algorithmic propagation is both \emph{sound} (it derives nothing beyond what the axioms entail) and \emph{complete} (it derives everything the axioms entail).
\end{itemize}

\subsection{Surprise Analysis}

For each free variable~$v$ and truth value $\textit{val} \in \{\top, \bot\}$, the \emph{surprise} of the hypothetical result $v = \textit{val}$ is:
\[
  \text{surprise}(v = \textit{val}) \;=\; \log_2 \frac{\text{MC}(\Sigma)}{\text{MC}(\Sigma \mid v = \textit{val})}
\]
\oliver{I've now thought about it, and this definition makes perfect sense! The quantity $-\log_2 p(E)$ is sometimes called \emph{surprisal} (or \emph{self-information}) of the event $E$, and the (Shannon) entropy of a random variable (indicator for the event) is exactly the expectation of the surprisal. Specifically, if we consider the uniform distribution over the models of $\Sigma$, then it is easy (and worthwhile) to check that your above definition of surprise is exactly computing the \emph{surprisal} of the event v=val. Maybe you already had this in mind during the meeting, but I did not follow all this if you did already have it in mind, and I think this additional context completely justifies the definition. (And, funnily, I don't think you really need to add any such justification in the writing, I'm more just glad that it exists, convincing me that this is the "right" definition/quantity to use.}
Higher surprise means the result eliminates a larger fraction of consistent worlds. Table~\ref{tab:surprise} lists the 20~most informative open problems. The cSDD language dominates the ranking because it is a recently introduced language with few known succinctness relationships, making each potential result highly constraining.

\begin{table}[ht]
\centering
\small
\caption{Top 20 most informative open problems by maximum surprise (bits). $\text{s}(\top)$ and $\text{s}(\bot)$ denote the surprise of the variable being true or false, respectively. By nature, it is highly unlikely that any of these results holds; however, if we could resolve even one, it would yield a large amount of information about the structure of the KC landscape.}
\label{tab:surprise}
\begin{tabular}{clrr}
\textbf{Rank} & \textbf{Variable} & $\mathbf{s(\top)}$ & $\mathbf{s(\bot)}$ \\ \hline
1 & $P(\text{dec-SDNNF} \to \text{cSDD})$ & 11.84 & 0.00 \\
2 & $P(\text{SDD} \to \text{cSDD})$ & 11.81 & 0.00 \\
3 & $P(\text{IP} \to \text{cSDD})$ & 11.55 & 0.00 \\
4 & $P(\text{uOBDD} \to \text{cSDD})$ & 11.35 & 0.00 \\
5 & $Q(\text{DNF} \to \text{cSDD})$ & 10.48 & 0.00 \\
6 & $P(\text{cSDD} \to \text{MODS})$ & 9.49 & 0.00 \\
7 & $Q(\text{d-SDNNF} \to \text{cSDD})$ & 8.00 & 0.01 \\
8 & $Q(\text{uOBDD} \to \text{cSDD})$ & 8.00 & 0.01 \\
9 & $Q(\text{IP} \to \text{cSDD})$ & 7.99 & 0.01 \\
10 & $Q(\text{SDD} \to \text{cSDD})$ & 7.15 & 0.01 \\
11 & $P(\text{nOBDD} \to \text{FBDD})$ & 6.89 & 0.01 \\
12 & $P(\text{SDNNF} \to \text{dec-DNNF})$ & 6.88 & 0.01 \\
13 & $P(\text{OBDD} \to \text{cSDD})$ & 6.87 & 0.01 \\
14 & $P(\text{DNF} \to \text{nOBDD})$ & 5.77 & 0.03 \\
15 & $P(\text{IP} \to \text{uOBDD})$ & 5.35 & 0.04 \\
16 & $Q(\text{cSDD} \to \text{FBDD})$ & 0.04 & 5.35 \\
17 & $Q(\text{cSDD} \to \text{dec-DNNF})$ & 0.04 & 5.35 \\
18 & $P(\text{OBDD}_< \to \text{cSDD})$ & 5.24 & 0.04 \\
19 & $Q(\text{OBDD} \to \text{cSDD})$ & 4.98 & 0.05 \\
20 & $Q(\text{dec-SDNNF} \to \text{cSDD})$ & 4.98 & 0.05 \\
\end{tabular}
\end{table}

\subsection{Research Value}

The surprise metric from Table~\ref{tab:surprise} ranks variables by how informative a single direction is, but many open problems are highly asymmetric: one outcome is very surprising while the other is nearly trivial.  To capture the value of investigating an open problem \emph{regardless of outcome}, we define the \textbf{research value}:
\[
  \text{RV}(v) \;=\; \text{surprise}(v = \top) \;\cdot\; \text{surprise}(v = \bot)
\]
A high research value means that both possible outcomes are roughly equally likely (in the sense that the model count of each outcome is similar), so resolving the problem is highly informative no matter the answer and worth investigating.

Unlike the max-surprise ranking, which is dominated by cSDD-related variables with one-sided surprise, the research value ranking surfaces \emph{balanced} open problems---those where the outcome is genuinely uncertain from the perspective of our current knowledge. All top entries have research values near~1.0, corresponding to roughly equal surprise in both directions.

\begin{table}[ht]
\centering
\small
\caption{Top 20 open problems by research value $\text{RV} = \text{s}(\top) \cdot \text{s}(\bot)$.}
\label{tab:rv}
\begin{tabular}{clrrr}
\textbf{Rank} & \textbf{Variable} & $\mathbf{s(\top)}$ & $\mathbf{s(\bot)}$ & \textbf{RV} \\ \hline
1 & $Q(\text{cSDD} \to \text{IP})$ & 0.98 & 1.02 & 1.00 \\
2 & $P(\text{cSDD} \to \text{DNF})$ & 1.09 & 0.91 & 1.00 \\
3 & $Q(\text{d-SDNNF} \to \text{SDD})$ & 0.90 & 1.11 & 1.00 \\
4 & $Q(\text{uOBDD} \to \text{SDD})$ & 0.90 & 1.11 & 1.00 \\
5 & $P(\text{IP} \to \text{SDNNF})$ & 0.88 & 1.13 & 1.00 \\
6 & $P(\text{uOBDD} \to \text{dec-DNNF})$ & 1.14 & 0.87 & 0.99 \\
7 & $Q(\text{cSDD} \to \text{OBDD}_<)$ & 1.15 & 0.86 & 0.99 \\
8 & $P(\text{cSDD} \to \text{FBDD})$ & 1.19 & 0.83 & 0.99 \\
9 & $P(\text{nOBDD} \to \text{d-DNNF})$ & 0.83 & 1.20 & 0.99 \\
10 & $P(\text{cSDD} \to \text{CNF})$ & 1.20 & 0.82 & 0.99 \\
11 & $Q(\text{MODS} \to \text{PI})$ & 0.81 & 1.22 & 0.99 \\
12 & $Q(\text{DNF} \to \text{nOBDD})$ & 1.25 & 0.79 & 0.98 \\
13 & $Q(\text{DNF} \to \text{SDNNF})$ & 1.25 & 0.79 & 0.98 \\
14 & $Q(\text{d-SDNNF} \to \text{FBDD})$ & 0.77 & 1.27 & 0.98 \\
15 & $Q(\text{d-SDNNF} \to \text{dec-DNNF})$ & 0.77 & 1.27 & 0.98 \\
16 & $Q(\text{uOBDD} \to \text{FBDD})$ & 0.77 & 1.27 & 0.98 \\
17 & $Q(\text{uOBDD} \to \text{dec-DNNF})$ & 0.77 & 1.27 & 0.98 \\
18 & $P(\text{SDD} \to \text{dec-DNNF})$ & 1.29 & 0.76 & 0.98 \\
19 & $P(\text{DNF} \to \text{nFBDD})$ & 1.29 & 0.76 & 0.98 \\
20 & $P(\text{SDD} \to \text{nFBDD})$ & 0.75 & 1.30 & 0.98 \\
\end{tabular}
\end{table}

\subsection{Comparison to Algorithmic Propagation}

The dedicated graph algorithms of Sections~3--4 remain preferable for the web application for two reasons: (1)~they produce \emph{informative witness paths}---human-readable chains of citations and intermediate edges explaining \emph{why} each derived fact holds---whereas a compiled representation only answers ``entailed or not''; and (2)~they are simpler and faster. The meta-compilation approach complements these algorithms by enabling queries they do not support (model counting, surprise quantification) and by providing an independent verification that the propagation is sound and complete.

\clearpage
% =============================
% Bibliography
% =============================
\bibliographystyle{plainnat}
\bibliography{refs}

\end{document}