# Graph Validation & Propagation (Level 1)

This document specifies the **Level‑1 semantic validator** and **propagator** for the Knowledge Compilation Map adjacency matrix.

Scope:
- Covers *only* succinctness/compilation edges (the adjacency matrix).
- Does **not** cover Level‑2 semantics (queries/transformations implication propagation).

Terminology:
- A directed edge is a `DirectedSuccinctnessRelation` stored in `adjacencyMatrix.matrix[i][j]`.
- Each edge has a `status` string.

**Design goal**: Users add only the claims made by paper authors; all trivial/transitive/implicit facts propagate automatically. This improves UX and simplifies data entry.

## Canonical statuses and intended meaning

The canonical dataset uses these statuses (see `relationTypes` in `src/lib/data/database.json`):

| Status | Meaning | Guarantees | Witness description phrase |
| --- | --- | --- | --- |
| `poly` | Polynomial transformation exists | poly exists; quasi exists | "in polynomial time" |
| `unknown-poly-quasi` | Quasi exists, poly unknown | quasi exists | "in at worst quasipolynomial time" |
| `no-poly-quasi` | Quasi exists and poly is impossible (optimal quasi) | quasi exists; poly does not exist | "in quasipolynomial time" |
| `unknown-both` | Both poly and quasi are unknown | none | *(cannot appear in witness path)* |
| `no-poly-unknown-quasi` | Poly is impossible; quasi unknown | poly does not exist | *(cannot appear in witness path)* |
| `no-quasi` | No quasi-polynomial transformation exists | quasi does not exist; poly does not exist | *(cannot appear in witness path)* |

Notes:
- Additional relation types exist in `database.json` (e.g. `unknown`, `not-poly`) but are **view-only** (generated by filters) and rejected by `validateDatasetStructure`.
- The diagonal `A → A` is never examined by the frontend. The validator/propagator should **completely ignore** diagonal entries (do not validate, do not traverse).

## `null` vs `unknown-both`

The matrix can have `null` (no edge stored) or an explicit `unknown-both` relation.

Semantic distinction:
- `null`: "We (app creators) haven't recorded anything about this pair."
- `unknown-both`: "This is an open problem; no one on Earth knows."

Algorithmic treatment:
- **Treat identically** for reachability and upgrade/downgrade logic.
- **Preserve identity**: `null` stays `null` and `unknown-both` stays `unknown-both` unless the propagator performs an upgrade or downgrade.
- If the propagator resolves an `unknown-both` edge (i.e., solves an open problem), emit an **alert** to the caller so this can be surfaced to the user.

## Level‑1 semantic model

We model each directed edge `A → B` as constraints about existence of two kinds of transforms:

- `P(A,B)`: a polynomial transform exists.
- `Q(A,B)`: a quasi-polynomial transform exists.

Assumptions (axioms):
- Poly implies quasi: `P(A,B) ⇒ Q(A,B)`.
- Transitivity under composition:
  - `P(A,B) ∧ P(B,C) ⇒ P(A,C)`
  - `Q(A,B) ∧ Q(B,C) ⇒ Q(A,C)`
  - Mixed composition is quasi: `P ∘ Q` and `Q ∘ P` imply quasi.

The Level‑1 validator is implemented using *reachability* on two directed graphs:
- `G_P`: edges where `P` is guaranteed (status `poly`).
- `G_Q`: edges where `Q` is guaranteed (statuses `poly`, `unknown-poly-quasi`, `no-poly-quasi`).

## Derived edge schema

When the propagator creates or upgrades an edge, it populates these fields:

| Field | Value |
| --- | --- |
| `status` | the upgraded status (`poly` or `unknown-poly-quasi`) |
| `description` | witness path as formatted string (see below) |
| `refs` | **union** of all `refs` arrays from edges in the witness path |
| `separatingFunctionIds` | `undefined` |
| `hidden` | `false` (derived edges are visible; hiding is a UI filter concern) |
| `derived` | `true` *(new field)* |

### Witness description format

Example: `"Derived: A transforms to B in polynomial time. B transforms to C in at worst quasipolynomial time."`

Use the "witness description phrase" from the status table above for each hop.

### New type field

Add to `DirectedSuccinctnessRelation`:

```ts
/** True if this edge was inferred by the propagator rather than manually authored. */
derived?: boolean;
```

This enables future filters like "hide all derived edges".

## Two validators: consistency vs closure

We maintain two related validators because they serve different UX roles.

### 1) `validateAdjacencyConsistency` (sound consistency)

Purpose:
- Detect **hard contradictions** (e.g. a `no-quasi` edge contradicts a discovered quasi path).
- Useful as an early guardrail and inside contradiction search.
- **Run before Phase 1** to reject invalid inputs early.

Algorithm:
1. Build `G_P` and `G_Q` from guaranteed edges.
2. Compute reachability with witnesses using **DFS** (parent pointers on the stack):
   - For each source `s`, run DFS on `G_P` to compute `reachP[s][*]` and reconstruct paths.
   - For each source `s`, run DFS on `G_Q` to compute `reachQ[s][*]` and reconstruct paths.
3. For each ordered pair `(A,C)` with `A ≠ C` (skip diagonal):
   - If `reachP[A][C]` is true and the stored status asserts "no poly" (e.g. `no-poly-unknown-quasi`, `no-poly-quasi`, `no-quasi`), return invalid with the witness path in `G_P`.
   - If `reachQ[A][C]` is true and the stored status asserts "no quasi" (`no-quasi`), return invalid with the witness path in `G_Q`.
4. **Fail-fast**: Return the **first** error encountered; do not accumulate all errors.

Witness format:
- Return a formatted description string, e.g.:
  - `"Contradiction: B transforms to C in polynomial time. C transforms to A in polynomial time. Therefore B→A must have poly, but B→A is marked no-quasi."`

Complexity:
- With `V` languages, `E` edges, and DFS-from-each-source, time is `O(V·(V+E))` for each of `G_P` and `G_Q`.

### 2) `validateAdjacencyClosure` (consistency + completion)

Purpose:
- Enforce that the matrix is a **fixed point** under Level‑1 implications.
- This is what users intuit as "the matrix already includes all trivial implied facts."

Algorithm:
- Compute reachability as above (DFS).
- For each `(A,C)` with `A ≠ C`:
  - If `reachP[A][C]` then the status must be exactly `poly`.
  - Else if `reachQ[A][C]` then status must be one of `{ poly, unknown-poly-quasi, no-poly-quasi }`.

Example (direct proof):
- If `A→B` has quasi and `B→C` is poly, then `A→C` must have quasi.
- Therefore `A→C = unknown-both` is invalid under closure.

This validator is what Phase 1 propagation aims to satisfy.

## Propagator: two phases

### Execution order

1. Run `validateAdjacencyConsistency` on input data. If invalid, abort with error.
2. Run Phase 1 (direct build).
3. Run Phase 2 (contradiction discovery).
4. Return propagated data + any alerts (e.g. open problems solved).

### Phase 1: Direct build (monotone upgrades)

Goal:
- Make the dataset satisfy `validateAdjacencyClosure` without guessing negative information.
- Only adds/strengthens **positive** information (poly/quasi existence).

Fixed-point loop (required):
```
changed = true
while changed:
    changed = false
    recompute reachQ, reachP via DFS-from-each-source
    for each (A, C) with A ≠ C:
        attempt upgrades (below); if any applied, changed = true
```

Upgrade rules:
- **Quasi upgrade**: If `reachQ[A][C]` and current status does *not* guarantee quasi:
  - `null` or `unknown-both` ⇒ set to `unknown-poly-quasi` (with `derived: true`, witness description, refs union).
  - `no-poly-unknown-quasi` ⇒ set to `no-poly-quasi`.
- **Poly upgrade**: If `reachP[A][C]` and current status is not `poly`:
  - `null`, `unknown-both`, `unknown-poly-quasi` ⇒ set to `poly`.

Important guard:
- If an upgrade would overwrite a status that asserts a contradiction (e.g. upgrading to `poly` when the cell is `no-poly-quasi`), the propagator should **stop and report an inconsistency**. This indicates the input data was semantically inconsistent (should have been caught by prior consistency check, but serves as a safety net).

Provenance:
- Use DFS parent pointers to reconstruct the witness path.
- Format description using the witness description phrases.
- Collect `refs` as the union of all edge refs along the path.

### Phase 2: Contradiction discovery (monotone downgrades)

Goal:
- Infer negative facts (`no-poly-*`, `no-quasi`) by showing that assuming a stronger edge leads to inconsistency.

Loop structure: **Repeat-until-stable**
```
changed = true
while changed:
    changed = false
    for each edge (A, C) in matrix:
        if tryDowngrade(A, C):
            changed = true
```

This is simpler to implement correctly than a worklist approach and avoids missing transitive domino effects.

#### `tryDowngrade(A, C)` logic

Depends on current status:

**Case: `null` or `unknown-both`**
1. Trial: temporarily set status to `poly`.
2. Run `validateAdjacencyConsistency`.
3. If inconsistent:
   - Downgrade to `no-poly-unknown-quasi` (preserve `null` → create new edge; preserve `unknown-both` → keep as `unknown-both` base but change status).
  - Record witness: `"Assuming A→C is poly produces contradiction: [witness]. Therefore A→C is not poly."`
  - Set `derived: true`; set `hidden: false`; set `separatingFunctionIds: undefined`; set `refs` to the witness-path union.
   - Continue to step 4.
4. Trial: temporarily set status to `unknown-poly-quasi` (assert quasi exists).
5. Run `validateAdjacencyConsistency`.
6. If inconsistent:
  - Downgrade to `no-quasi`.
  - Record witness; set `derived: true`; set `hidden: false`; set `refs` to witness-path union.
   - Return true (changed).
7. If neither trial failed, return false.

**Case: `unknown-poly-quasi`**
1. Trial: temporarily set status to `poly`.
2. If inconsistent:
  - Downgrade to `no-poly-quasi`.
  - Record witness; set `derived: true`; set `hidden: false`; set `refs` to witness-path union.
   - Return true.
3. Return false.

**Case: `no-poly-unknown-quasi`**
1. Trial: temporarily set status to `no-poly-quasi` (assert quasi exists).
2. If inconsistent:
  - Downgrade to `no-quasi`.
  - Record witness; set `derived: true`; set `hidden: false`; set `refs` to witness-path union.
   - Return true.
3. Return false.

**All other statuses**: Return false (nothing to downgrade).

#### Witness format for downgrades

Example: `"Derived (contradiction): Assuming A→C is poly leads to: B transforms to A in polynomial time, A transforms to C in polynomial time, implying B→C is poly. But B→C is marked no-poly-quasi. Therefore A→C cannot be poly."`

Termination and runtime:
- Each edge can downgrade at most 2 times (see ladder).
- Each iteration scans all `O(E)` edges.
- Total: `O(E² · V²)` worst case, but with `V ≈ 20` this is still sub-second.

## Algorithmic Notes: Potential Optimizations

### Upgrade Optimization (Future Work)

The current Phase 1 upgrade algorithm recomputes full reachability (`DFS-from-each-source`) on every iteration. A simpler and potentially faster approach uses direct transitive closure:

```
// Simplified upgrade without explicit reachability computation
do until no changes:
    for each language A:
        for each language B:
            for each language C:
                if A→B exists and B→C exists:
                    if A→C does not exist or is weaker:
                        upgrade A→C = compose(A→B, B→C)
```

**Analysis**: This is essentially Floyd-Warshall for transitive closure.
- Time per iteration: $O(V^3)$ 
- Current DFS approach: $O(V \cdot (V + E))$ per iteration, but requires tracking parent pointers

For small graphs ($V \approx 20$), both approaches are sub-millisecond. The Floyd-Warshall style may be simpler to implement and reason about, though it loses explicit witness paths (would need separate path reconstruction).

### Downgrade Optimization (Future Work)

The current Phase 2 downgrade algorithm uses trial-and-error with the full consistency validator:
```
for each edge (A, C):
    temporarily assume stronger status
    run validateAdjacencyConsistency  // O(V·(V+E))
    if contradiction: downgrade
```

**Current runtime**: The validator runs DFS from each source, so `validateAdjacencyConsistency` is $O(V \cdot (V + E))$. Since we call it for every edge, the total is $O(E \cdot V \cdot (V + E))$ per iteration, or $O(E^2 \cdot V) for dense graphs like this one$.

A more efficient direct approach avoids the validator entirely:

```
// Optimized downgrade without validator calls
while changes remain:
    for each edge (A, C) marked no-poly (resp. no-quasi):
        for each language B:
            // Case 1: (A, B) is poly and (B, C) is unknown
            if (A, B) is poly and (B, C).poly is unknown:
                downgrade (B, C) to no-poly
                // Because A→B→C would be poly, but A→C is no-poly
            
            // Case 2: (A, B) is unknown and (B, C) is poly
            if (A, B).poly is unknown and (B, C) is poly:
                downgrade (A, B) to no-poly
                // Because A→B→C would be poly, but A→C is no-poly
```

**Optimized runtime**: $O(E \cdot V)$ per iteration, since for each "no-poly" edge we scan all intermediate languages.

**Trade-off**: The optimized version loses the explicit witness paths that come from the validator's DFS. For the KC Map's scale ($V \approx 20$), the current implementation is fast enough, so this optimization is left for future work when scaling to larger graphs.

## Implementation placement

- Level‑0 (structural) validation: `validateDatasetStructure` in `src/lib/data/validation.ts` (already exists).
- Level‑1 semantic validation: add `validateAdjacencyConsistency` and `validateAdjacencyClosure` in `src/lib/data/semantic-validation.ts` (new file).
- Propagator: extend or replace `propagateImplicitRelations` in `src/lib/data/propagation.ts` with the two-phase algorithm.
- Type update: add `derived?: boolean` to `DirectedSuccinctnessRelation` in `src/lib/types.ts`.

## Return types

```ts
interface SemanticValidationResult {
  ok: boolean;
  /** If not ok, a formatted description of the contradiction */
  error?: string;
  /** The witness path as language IDs (for programmatic use) */
  witnessPath?: string[];
}

interface PropagationResult {
  data: GraphData;
  /** Number of edges upgraded in Phase 1 */
  upgradesApplied: number;
  /** Number of edges downgraded in Phase 2 */
  downgradesApplied: number;
  /** Alerts, e.g. if an open problem (unknown-both) was resolved */
  alerts: string[];
}
```

## Level 2: Query Propagation

This section specifies the **Level‑2 propagator** for query operations. Query propagation runs **after** Level‑1 succinctness propagation is complete, using the computed reachability matrices.

### Theoretical Foundation

**Definition**: A *query* on a language $L$ is a function $q: L \to \{0,1\}$ that answers a decision problem about formulas in that language.

**Lemma 1 (Succinctness Composition)**: If there exists a polynomial (resp. quasipolynomial) transformation $m: L_1 \to L_2$ and a query $q: L_2 \to \{0,1\}$ is computable in polynomial (resp. quasipolynomial) time, then the composed function $q \circ m: L_1 \to \{0,1\}$ is also computable in polynomial (resp. quasipolynomial) time.

**Corollary**: If $L_1$ transforms to $L_2$ and $L_2$ supports query $q$, then $L_1$ also supports query $q$ with complexity at most $\max(\text{transform complexity}, \text{query complexity})$.

**Lemma 2 (Operation Implication)**: Certain operations imply others within the same language. These lemmas can involve both queries AND transformations in their antecedent/consequent:

Examples:
- Model Enumeration (ME) implies Model Counting (CT): enumerate all models and count
- Model Counting (CT) implies Consistency (CO): check if count $> 0$
- Validity (VA) + Negation (¬C) implies Consistency (CO): $\neg\phi$ valid iff $\phi$ unsatisfiable
- Model Enumeration (ME) implies Implicant (IM): enumerate until you find one

These lemmas are parameterized: $\{o_1, o_2, \ldots, o_n\} \Rightarrow o_m$ where each $o_i$ can be a query or transformation. If a language supports all operations in the antecedent set at a given complexity, it supports the consequent at that complexity.

### Operation Implication Lemmas

Lemmas are stored in `database.json` under `operationLemmas` (not hardcoded). Each lemma specifies:
- `id`: unique identifier
- `antecedent`: list of operation codes (queries like "CT" or transformations like "NOT_C")
- `consequent`: the implied operation code
- `description`: human-readable justification
- `refs`: supporting references

Example lemmas:

| ID | Antecedent | Consequent | Justification |
| --- | --- | --- | --- |
| `me-ct` | ME | CT | Count models by enumeration |
| `ct-co` | CT | CO | Satisfiable iff model count $> 0$ |
| `va-not-co` | VA, ¬C | CO | Check validity of $\neg\phi$; valid iff $\phi$ unsatisfiable |
| `me-im` | ME | IM | Return any enumerated model as implicant |
| `se-ce` | SE | CE | Clausal entailment is a special case of sentential entailment |

### Complexity Lattice for Queries

The same complexity codes are used for queries and succinctness relations:
- `poly` - query is polynomial
- `unknown-poly-quasi` - polynomial unknown, quasi-polynomial exists
- `no-poly-quasi` - polynomial impossible, quasi-polynomial exists
- `no-poly-unknown-quasi` - polynomial impossible, quasi unknown
- `no-quasi` - quasi-polynomial impossible (implies polynomial also impossible)
- `unknown-both` - both polynomial and quasi-polynomial unknown
- `unknown-to-us` - not yet researched (treat as `null` for propagation)

### Partial Derivation

Like succinctness relations with `noPolyDescription`/`quasiDescription`, operation support needs partial derivation tracking. When an operation has `no-poly-quasi` complexity, we track separately:
- Whether the "no poly" claim was manually authored or derived
- Whether the "quasi exists" claim was manually authored or derived

The `derived` field on `KCOpSupport` is `true` only if BOTH claims are derived.

### Phase 3: Query Propagation via Succinctness (Upgrades)

**Precondition**: Level‑1 succinctness propagation is complete; `reachP` and `reachQ` matrices are stable.

Since the reachability matrices already encode all transitive closures, no graph traversal is needed. The algorithm simply iterates over all pairs.

**Algorithm**:
```
propagateQueriesViaSuccinctness(reachP, reachQ, languages):
    changed = false
    for each query q in QUERIES:
        for each language L2 in languages:
            qComplexity = L2.queries[q].complexity
            if qComplexity guarantees poly:
                for each language L1 in languages:
                    if reachP[L1][L2]:  // poly path exists
                        if L1.queries[q].complexity does not guarantee poly:
                            upgrade L1.queries[q] to poly
                            set L1.queries[q].derived = true
                            set L1.queries[q].description = witness path + " Therefore {L1} supports {q} in polynomial time."
                            changed = true
            if qComplexity guarantees quasi:
                for each language L1 in languages:
                    if reachQ[L1][L2]:  // quasi path exists
                        if L1.queries[q].complexity does not guarantee quasi:
                            // Upgrade to quasi; preserve poly status
                            upgrade L1.queries[q] to at-least-quasi
                            set L1.queries[q].derived = true (if wasn't already manual)
                            changed = true
    return changed
```

**Complexity**: $O(Q \cdot V^2)$ where $Q$ = number of queries, $V$ = number of languages.

### Phase 4: Query Propagation via Lemmas (Upgrades)

**Algorithm**:
```
propagateQueriesViaLemmas(languages, lemmas):
    changed = false
    for each language L in languages:
        for each lemma (antecedent, consequent) in lemmas:
            // antecedent may include both queries and transformations
            if all operations in antecedent are supported by L:
                worst_complexity = max complexity among antecedent operations
                if L.operations[consequent].complexity > worst_complexity:
                    upgrade L.operations[consequent] to worst_complexity
                    set derived = true
                    set description = "By {lemma}: since {L} supports {antecedent}, it supports {consequent}."
                    changed = true
    return changed
```

### Phase 3+4 Combined: Fixed-Point Query Upgrade Loop

The upgrade phases run in a fixed-point loop, processing polynomial before quasipolynomial:

```
propagateQueryUpgrades(data, reachP, reachQ, lemmas):
    // First: all poly upgrades until stable
    polyChanged = true
    while polyChanged:
        polyChanged = false
        if propagateQueriesViaSuccinctness(reachP, null, data.languages, poly_only=true):
            polyChanged = true
        if propagateQueriesViaLemmas(data.languages, lemmas, poly_only=true):
            polyChanged = true
    
    // Second: all quasi upgrades until stable
    quasiChanged = true
    while quasiChanged:
        quasiChanged = false
        if propagateQueriesViaSuccinctness(null, reachQ, data.languages, quasi_only=true):
            quasiChanged = true
        if propagateQueriesViaLemmas(data.languages, lemmas, quasi_only=true):
            quasiChanged = true
    
    return data
```

### Phase 5: Query Downgrades (Contradiction Discovery)

**Key Insight**: Unlike succinctness downgrades which require exhaustive trial-and-error with the validator, query downgrades can be computed efficiently without graph traversal.

**Contrapositive of Lemma 1**: If $L_1$ does NOT support query $q$, and there's a polynomial transformation $L_2 \to L_1$, then $L_2$ does NOT support $q$ in polynomial time either.

In the language of chains: consider `L_1 → L_2 → ... → L_i → (unknown) → q` where `L_1` does not support `q`. Since succinctness is transitive and already propagated, we have `L_1 → L_i` directly. Thus we only need to check length-1 chains.

**Simplified Downgrade Algorithm** (no graph traversal needed):
```
propagateQueryDowngrades(reachP, reachQ, languages):
    changed = false
    for each query q in QUERIES:
        for each language Li with unknown q support:
            for each language L1 in languages:
                if reachP[L1][Li] and L1.queries[q] asserts "no poly":
                    downgrade Li.queries[q] to "no poly"
                    set derived = true
                    set description = "L1 does not support q in poly, and L1 transforms to Li in poly. Therefore Li does not support q in poly."
                    changed = true
                    break  // found a witness, move to next Li
            
            for each language L1 in languages:
                if reachQ[L1][Li] and L1.queries[q] asserts "no quasi":
                    downgrade Li.queries[q] to "no quasi"
                    set derived = true
                    changed = true
                    break
    return changed
```

**Complexity**: $O(Q \cdot V^2)$ — same as upgrades, no expensive validator calls.

### Query Consistency Validator

The query validator checks for contradictions where the data claims:
- Language $L_1$ supports $q$ in poly, AND
- $L_1 \xrightarrow{poly} L_2$, BUT
- $L_2$ is marked as NOT supporting $q$ in poly

This mirrors the succinctness consistency validator but treats the query as the final node in the chain: `L_1 \to L_2 \to ... \to L_n \to q`.

**Algorithm**:
```
validateQueryConsistency(reachP, reachQ, languages):
    for each query q in QUERIES:
        for each language L1 in languages:
            for each language L2 in languages:
                if reachP[L1][L2]:
                    if L1.queries[q] guarantees poly and L2.queries[q] asserts "no poly":
                        return { ok: false, error: "Contradiction: L1 supports q in poly, transforms to L2 in poly, but L2 doesn't support q in poly" }
                if reachQ[L1][L2]:
                    if L1.queries[q] guarantees quasi and L2.queries[q] asserts "no quasi":
                        return { ok: false, error: "..." }
    return { ok: true }
```

### Implementation Placement

- Query propagation: add `propagateQueryOperations` in `src/lib/data/propagation.ts`
- Operation lemmas: store in `database.json` under `operationLemmas`
- Type updates: extend `KCOpSupport` with `description?: string` and `derived?: boolean`
- Query validator: add `validateQueryConsistency` alongside `validateAdjacencyConsistency`

### Execution Order

1. Run Level-1 succinctness propagation (Phases 0, 1, 2)
2. Run Level-2 query validation (check consistency before propagation)
3. Run Level-2 query propagation (Phases 3, 4, 5)
4. Return combined results

### Note on Transformation Operations

Transformation operations (CD, FO, ∧C, etc.) are NOT propagated via succinctness. Unlike queries where $L_1 \to L_2$ helps compute $q$ on $L_1$, a transformation $T: L \to L$ on one language does not help construct $T': L' \to L'$ on another language. This would require $L$ and $L'$ to be bidirectionally equivalent, which is rare.

However, transformation operations CAN participate in lemma-based propagation as antecedents (e.g., VA + ¬C → CO).

### Note on Level-1 Downgrade Efficiency

The current Level-1 succinctness downgrade algorithm uses exhaustive trial-and-error:
```
for each edge:
    temporarily assume stronger status
    run full validator
    if contradiction: downgrade
```

This is $O(E \cdot V \cdot (V + E))$ per iteration. A more efficient approach would use the same "direct witness" pattern as query downgrades, avoiding repeated validator calls. This optimization is left for future work; the current approach is correct and fast enough for small graphs ($V \approx 20$).
